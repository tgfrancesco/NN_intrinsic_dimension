{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "former-faculty",
   "metadata": {},
   "source": [
    "# Extraction of the parameters of the neural networks generated by NN_generation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aggregate-apparatus",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "affecting-singapore",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading one model to obtein the numbers of layers\n",
    "from keras.models import load_model\n",
    "model = load_model('E:/Ensemble/Immagini distorte/solo distorsione/epoca1-200/Checkpoint/model_NN0_ep0.hdf5')\n",
    "num_layers = len(model.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "velvet-involvement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "supreme-signature",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_extraction(model, l):\n",
    "    \"\"\"\n",
    "    input:  model -> network from which to extract the parameters\n",
    "            l -> layer from which to extract the parameters of the network \"model\"\n",
    "            \n",
    "    output: concatenation -> vector obtained by concatenating weights and biases of layer l:\n",
    "                             [weights_flatten]U[bias])[layer[l] \n",
    "    \"\"\"\n",
    "    import h5py\n",
    "    weights = model.layers[l].get_weights()[0]\n",
    "    biases = model.layers[l].get_weights()[1]\n",
    "    concatenation = np.concatenate((weights.flatten(),biases))\n",
    "    return concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "electrical-resolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NeuralNet_matrix(e, num_NN, num_layers):\n",
    "    \"\"\"\n",
    "    input:  e -> epoch that is being investigated\n",
    "            num_NN -> number of neural networks analyzed\n",
    "            num_layers -> number of layers making up the networks\n",
    "    \n",
    "    output: NNs_matrix -> num_NN x D matrix with D number of parameters (weights and biases) making up the network.\n",
    "                          Each row is thus formed:\n",
    "                          ([weights_flatten]U[bias])[layer[1])U([weights_flatten]U[bias])[layer[2])...\n",
    "    \"\"\"\n",
    "    \n",
    "    NNs_matrix = []\n",
    "    for i in range(num_NN):\n",
    "        NN_parametres = np.array([])\n",
    "        model = load_model('E:\\Ensemble\\Immagini distorte\\solo distorsione\\epoca1-200\\Checkpoint/model_NN'+str(i)+'_ep'+str(e)+'.hdf5')\n",
    "        for l in range(1,num_layers,2):                    # 2 is for overcoming the dropout layer                 \n",
    "            concatenation = data_extraction(model, l)   \n",
    "            NN_parametres = np.append(NN_parametres,concatenation)\n",
    "        NNs_matrix.append(NN_parametres)\n",
    "    NNs_matrix_np = np.array(NNs_matrix)\n",
    "    return NNs_matrix_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "sized-proposal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import TwoNN di A. Laio\n",
    "import Two_NN \n",
    "T = Two_NN.TwoNN(discard=0.1,block_analysis=False)\n",
    "\n",
    "# import MLE di Levina e Bickel\n",
    "import pandas as pd\n",
    "from geomle import mle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "mature-runner",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "num_NN = 100\n",
    "\n",
    "NN_parametres = np.array([])\n",
    "NNs_matrix = []\n",
    "\n",
    "# TwoNN result\n",
    "TNN_res = []                    \n",
    "TNN_ID = np.zeros(epochs)      # vector containing the IDs estimated by TwoNN for each epoch\n",
    "\n",
    "# MLE result\n",
    "MLE_ID = np.zeros(epochs)      # vector containing the IDs estimated by MLE for each epoch\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(0,epochs):\n",
    "    # load Neural Networks parametres\n",
    "    data_NNs = NeuralNet_matrix(epoch+1, num_NN, num_layers)\n",
    "    \n",
    "    # saving the matrix of the networks at the epoch time in format (D x num_NN)\n",
    "    # to estimate the ID with FastDANCo and MiND in Matlab\n",
    "    arr_matlab = data_NNs.transpose()\n",
    "    np.savetxt(\"E:/Ensemble/Immagini distorte/solo distorsione/epoca1-200/Matlab/mat_ep\"+str(epoch+1)+\".txt\", arr_matlab, delimiter=',')\n",
    "\n",
    "    # apply TwoNN\n",
    "    TNN_res = T.fit(data_NNs)\n",
    "    TNN_ID[epoch] = TNN_res.DimEstimate_\n",
    "    \n",
    "    # apply MLE    \n",
    "    MLE_data = pd.DataFrame(data_NNs)\n",
    "    MLE_res_temp = mle(MLE_data)[0]\n",
    "    MLE_ID[epoch] = MLE_res_temp.mean()\n",
    "    \n",
    "\n",
    "    \n",
    "# creating the dataframe with the results just found\n",
    "Results = pd.DataFrame()\n",
    "Results['Epoch'] = [e for e in range(0,epochs)]\n",
    "Results['TwoNN ID'] = TNN_ID\n",
    "Results['MLE ID'] = MLE_ID\n",
    "\n",
    "# saving the dataframe in order to update it with FastDANCo and MiND data from Matlab\n",
    "Results.to_excel(\"E:/Ensemble/Immagini distorte/solo distorsione/epoca1-200/soloDistorsione_model_sub_ID_finali.xlsx\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
