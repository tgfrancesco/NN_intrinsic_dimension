{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "balanced-tunnel",
   "metadata": {},
   "source": [
    "# Generation of num_NN neural networks trained for num_epochs epochs on FASHION-MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "organized-means",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_NN = 100              # number of neural networks to be generated\n",
    "num_epochs = 200       # number of epochs for which to train the networks\n",
    "b_size = 32               # batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "united-isolation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get FASHION-MNIST data\n",
    "from keras.datasets import fashion_mnist\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# data reshaping\n",
    "X_train = X_train.reshape(X_train.shape[0],28*28)\n",
    "X_test = X_test.reshape(X_test.shape[0],28*28)\n",
    "\n",
    "# data normalization\n",
    "X_train = X_train/X_train.max()\n",
    "X_test = X_test/X_test.max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "monthly-serial",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 784), (10000, 784))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "worth-garbage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding of the label\n",
    "from keras.utils import to_categorical\n",
    "num_classes = 10\n",
    "y_train_dummy = to_categorical(y_train,num_classes)\n",
    "y_test_dummy = to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "conventional-assist",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.], dtype=float32))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0], y_train_dummy[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "respected-retrieval",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to create the directory in which to save the files\n",
    "def create_dir(path):\n",
    "    if not os.path.exists(os.path.dirname(path)):\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(path))\n",
    "        except OSError as exc: # Guard against race condition\n",
    "            if exc.errno != errno.EEXIST:\n",
    "                raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serial-brush",
   "metadata": {},
   "source": [
    "## Creation of a basic model to be called to have the same initialization of the networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exposed-asian",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "\n",
    "model = Sequential([Dropout(0, input_dim=X_train.shape[1]),\n",
    "                    Dense(512, activation='relu'),\n",
    "                    Dropout(0),\n",
    "                    Dense(128,activation='relu'),\n",
    "                    Dropout(0),\n",
    "                    Dense(num_classes,activation='softmax')])\n",
    "\n",
    "model.compile(optimizer='sgd',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "path_same_init = 'E:/Fonti di Variazione/test approssimazione salvataggio/NN_epoca0_copia_'+str(i)+'.hdf5'\n",
    "create_dir(path_same_init)\n",
    "model.save(path_same_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virtual-facing",
   "metadata": {},
   "source": [
    "## Random Initialization ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frequent-attribute",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluated = []            # list containing the accuracy and loss results of each network at the end of the training\n",
    "\n",
    "for i in range(num_NN):\n",
    "    # Checkpoint\n",
    "    cwd = os.getcwd()\n",
    "    \n",
    "    # path in which to save the parameters (weights and biases) of the i-th network\n",
    "    path_parametres = 'E:/Fonti di Variazione/Rand. Init./epoca0-200/Checkpoint/model_NN'+str(i)+'_ep{epoch:d}.hdf5'\n",
    "    \n",
    "    # path in which to save the loss and accuracy of each network at the end of each epoch\n",
    "    path_history = 'E:/Fonti di Variazione/Rand. Init./epoca0-200/loss_accuracy/history'+str(i)+'.xlsx'\n",
    "    \n",
    "    # creation of directories associated with paths\n",
    "    create_dir(path_parametres)   \n",
    "    create_dir(path_history)\n",
    "    \n",
    "    \n",
    "    # checkpoint necessary to save the parameters of the i-th network at the end of each epoch\n",
    "    my_checkpoint = [\n",
    "        ModelCheckpoint(filepath = path_parametres, monitor='val_loss', \n",
    "                        verbose=0, save_best_only=False, save_weights_only=False, \n",
    "                        mode='auto', save_freq='epoch')\n",
    "    ]\n",
    "\n",
    "    \n",
    "    # creation of networks with random initialization\n",
    "    \n",
    "    model = Sequential([Dropout(0, input_dim=X_train.shape[1]),\n",
    "                    Dense(512, activation='relu'),\n",
    "                    Dropout(0),\n",
    "                    Dense(128,activation='relu'),\n",
    "                    Dropout(0),\n",
    "                    Dense(num_classes,activation='softmax')])\n",
    "    \n",
    "    model.compile(optimizer='sgd',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    \n",
    "    # to save the network before starting training\n",
    "    model.save('E:/Fonti di Variazione/Rand. Init./epoca0-200/Checkpoint/model_NN'+str(i)+'_ep0.hdf5')\n",
    "    \n",
    "    \n",
    "    # network fit process with performance saving on the training set\n",
    "    pd.DataFrame(model.fit(X_train, y_train_dummy, validation_split=0.2, epochs=num_epochs, \n",
    "                batch_size=b_size, shuffle=False,callbacks=[my_checkpoint]).history).to_excel(path_history)\n",
    "    \n",
    "    \n",
    "    evaluated.append(model.evaluate(X_test, y_test_dummy))\n",
    "\n",
    "path_evaluation = 'E:/Fonti di Variazione/Rand. Init./epoca0-200/Evaluation/evaluated_models.xlsx'\n",
    "create_dir(path_evaluation)\n",
    "pd.DataFrame(evaluated).to_excel(path_evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welcome-piano",
   "metadata": {},
   "source": [
    "## Batch Shuffle ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flexible-candy",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluated = []            # list containing the accuracy and loss results of each network at the end of the training\n",
    "\n",
    "\n",
    "for i in range(num_NN):\n",
    "    # Checkpoint\n",
    "    cwd = os.getcwd()\n",
    "    \n",
    "    # path in which to save the parameters (weights and biases) of the i-th network\n",
    "    path_parametres = 'E:/Fonti di Variazione/Batch Shuffle 32/epoca0-200/Checkpoint/model_NN'+str(i)+'_ep{epoch:d}.hdf5'\n",
    "    \n",
    "    # path in which to save the loss and accuracy of each network at the end of each epoch\n",
    "    path_history = 'E:/Fonti di Variazione/Batch Shuffle 32/epoca0-200/loss_accuracy/history'+str(i)+'.xlsx'\n",
    "    \n",
    "    # creation of directories associated with paths\n",
    "    create_dir(path_parametres)   \n",
    "    create_dir(path_history)\n",
    "    \n",
    "    \n",
    "    # checkpoint necessary to save the parameters of the i-th network at the end of each epoch\n",
    "    my_checkpoint = [\n",
    "        ModelCheckpoint(filepath = path_parametres, monitor='val_loss', \n",
    "                        verbose=0, save_best_only=False, save_weights_only=False, \n",
    "                        mode='auto', save_freq='epoch')\n",
    "    ]\n",
    "\n",
    "    # to create networks that are always initialized in the same way\n",
    "    model = load_model(path_same_init)\n",
    "    \n",
    "    model.compile(optimizer='sgd',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    \n",
    "    # to save the network before starting training\n",
    "    model.save('E:/Fonti di Variazione/Batch Shuffle 32/epoca0-200/Checkpoint/model_NN'+str(i)+'_ep0.hdf5')\n",
    "    \n",
    "    \n",
    "    # network fit process with performance saving on the training set\n",
    "    pd.DataFrame(model.fit(X_train, y_train_dummy, validation_split=0.2, epochs=num_epochs, \n",
    "                batch_size=b_size, shuffle=True,callbacks=[my_checkpoint]).history).to_excel(path_history)\n",
    "    \n",
    "    \n",
    "    evaluated.append(model.evaluate(X_test, y_test_dummy))\n",
    "\n",
    "path_evaluation = 'E:/Fonti di Variazione/Batch Shuffle 32/epoca0-200/Evaluation/evaluated_models.xlsx'\n",
    "create_dir(path_evaluation)\n",
    "pd.DataFrame(evaluated).to_excel(path_evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "palestinian-chapter",
   "metadata": {},
   "source": [
    "## Dropout ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rental-italy",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluated = []            # list containing the accuracy and loss results of each network at the end of the training\n",
    "\n",
    "\n",
    "for i in range(num_NN):\n",
    "    # Checkpoint\n",
    "    cwd = os.getcwd()\n",
    "    \n",
    "    # path in which to save the parameters (weights and biases) of the i-th network\n",
    "    path_parametres = 'E:/Fonti di Variazione/Dropout 0.5/epoca0-200/Checkpoint/model_NN'+str(i)+'_ep{epoch:d}.hdf5'\n",
    "    \n",
    "    # path in which to save the loss and accuracy of each network at the end of each epoch\n",
    "    path_history = 'E:/Fonti di Variazione/Dropout 0.5/epoca0-200/loss_accuracy/history'+str(i)+'.xlsx'\n",
    "    \n",
    "    # creation of directories associated with paths\n",
    "    create_dir(path_parametres)   \n",
    "    create_dir(path_history)\n",
    "    \n",
    "    \n",
    "    # checkpoint necessary to save the parameters of the i-th network at the end of each epoch\n",
    "    my_checkpoint = [\n",
    "        ModelCheckpoint(filepath = path_parametres, monitor='val_loss', \n",
    "                        verbose=0, save_best_only=False, save_weights_only=False, \n",
    "                        mode='auto', save_freq='epoch')\n",
    "    ]\n",
    "\n",
    "    # to create networks that are always initialized in the same way\n",
    "    model = load_model(path_same_init)\n",
    "\n",
    "    model.compile(optimizer='sgd',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "    # to modify the dropout rates of the loaded network starting from the fixed initialization\n",
    "    model.layers[0].rate = 0.5\n",
    "    model.layers[2].rate = 0.5\n",
    "    model.layers[4].rate = 0.5\n",
    "    \n",
    "    # to save the network before starting training\n",
    "    model.save('E:/Fonti di Variazione/Dropout 0.5/epoca0-200/Checkpoint/model_NN'+str(i)+'_ep0.hdf5')\n",
    "    \n",
    "    # network fit process with performance saving on the training set\n",
    "    pd.DataFrame(model.fit(X_train, y_train_dummy, validation_split=0.2, epochs=num_epochs, \n",
    "                batch_size=b_size, shuffle=False,callbacks=[my_checkpoint]).history).to_excel(path_history)\n",
    "    \n",
    "    \n",
    "    evaluated.append(model.evaluate(X_test, y_test_dummy))\n",
    "\n",
    "path_evaluation = 'E:/Fonti di Variazione/Dropout 0.5/epoca0-200/Evaluation/evaluated_models.xlsx'\n",
    "create_dir(path_evaluation)\n",
    "pd.DataFrame(evaluated).to_excel(path_evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "static-niger",
   "metadata": {},
   "source": [
    "## Distorted ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transsexual-commons",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the arguments\n",
    "rotation_range_val = 5\n",
    "width_shift_val = 0.25\n",
    "height_shift_val = 0.25\n",
    "shear_range_val = 45\n",
    "zoom_range_val=[1.0,1.5]\n",
    "# import relevant library\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "for i in range(num_NN):\n",
    "    # Checkpoint\n",
    "    cwd = os.getcwd()\n",
    "    \n",
    "    # path in which to save the parameters (weights and biases) of the i-th network\n",
    "    path_parametres = 'E:/Fonti di Variazione/Distorted/epoca0-200/Checkpoint/model_NN'+str(i)+'_ep{epoch:d}.hdf5'\n",
    "    \n",
    "    # path in which to save the loss and accuracy of each network at the end of each epoch\n",
    "    path_history = 'E:/Fonti di Variazione/Distorted/epoca0-200/loss_accuracy/history'+str(i)+'.xlsx'\n",
    "    \n",
    "    # creation of directories associated with paths\n",
    "    create_dir(path_parametres)   \n",
    "    create_dir(path_history)\n",
    "    \n",
    "    \n",
    "    # distorted dataset creation\n",
    "    datagen = ImageDataGenerator(rotation_range=rotation_range_val, width_shift_range=width_shift_val,\n",
    "                             height_shift_range=height_shift_val, zoom_range=zoom_range_val,\n",
    "                             shear_range=shear_range_val)\n",
    "\n",
    "    datagen.fit(X_train.reshape(X_train.shape[0], 28, 28, 1))\n",
    "    data_dist = datagen.flow(X_train.reshape(X_train.shape[0], 28, 28, 1),y_train.reshape(y_train.shape[0], 1),\n",
    "                      batch_size=X_train.shape[0],shuffle=False)\n",
    "    \n",
    "    X, y = data_dist.next()\n",
    "    X_dist = X.reshape(X.shape[0], 28, 28)\n",
    "    y_dist = y.reshape(y.shape[0])\n",
    "    \n",
    "    # data reshaping\n",
    "    X_dist = X_dist.reshape(X_dist.shape[0],28*28)\n",
    "\n",
    "    # data normalization\n",
    "    X_dist = X_dist/X_dist.max()\n",
    "    \n",
    "    # One-hot encoding of the label\n",
    "    y_dist_dummy = to_categorical(y_dist,num_classes)\n",
    "    y_val_dummy = to_categorical(y_val,num_classes)\n",
    "    y_test_dummy = to_categorical(y_test, num_classes)\n",
    "    \n",
    "    \n",
    "    # checkpoint necessary to save the parameters of the i-th network at the end of each epoch\n",
    "    my_checkpoint = [\n",
    "        ModelCheckpoint(filepath = path_parametres, monitor='val_loss', \n",
    "                        verbose=0, save_best_only=False, save_weights_only=False, \n",
    "                        mode='auto', save_freq='epoch')\n",
    "    ]\n",
    "    \n",
    "\n",
    "    # to create networks that are always initialized in the same way\n",
    "    model = load_model(path_same_init)\n",
    "    \n",
    "    model.compile(optimizer='sgd',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    \n",
    "    # to save the network before starting training\n",
    "    model.save('E:/Fonti di Variazione/Distorted/epoca0-200/Checkpoint/model_NN'+str(i)+'_ep0.hdf5')\n",
    "    \n",
    "    # network fit process with performance saving on the training set\n",
    "    pd.DataFrame(model.fit(X_dist, y_dist_dummy, validation_data=(X_val, y_val_dummy), epochs=num_epochs, \n",
    "                batch_size=b_size, shuffle=False,callbacks=[my_checkpoint]).history).to_excel(path_history)\n",
    "    \n",
    "    \n",
    "    evaluated.append(model.evaluate(X_test, y_test_dummy))\n",
    "\n",
    "path_evaluation = 'E:/Fonti di Variazione/Distorted/epoca0-200/Evaluation/evaluated_models.xlsx'\n",
    "create_dir(path_evaluation)\n",
    "pd.DataFrame(evaluated).to_excel(path_evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strange-mexico",
   "metadata": {},
   "source": [
    "## Rand. Init. + BS ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governing-member",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluated = []            # list containing the accuracy and loss results of each network at the end of the training\n",
    "\n",
    "\n",
    "for i in range(num_NN):\n",
    "    # Checkpoint\n",
    "    cwd = os.getcwd()\n",
    "    \n",
    "    # path in which to save the parameters (weights and biases) of the i-th network\n",
    "    path_parametres = 'E:/Fonti di Variazione/Rand. Init. + BS/epoca0-200/Checkpoint/model_NN'+str(i)+'_ep{epoch:d}.hdf5'\n",
    "    \n",
    "    # path in which to save the loss and accuracy of each network at the end of each epoch\n",
    "    path_history = 'E:/Fonti di Variazione/Rand. Init. + BS/epoca0-200/loss_accuracy/history'+str(i)+'.xlsx'\n",
    "    \n",
    "    # creation of directories associated with paths\n",
    "    create_dir(path_parametres)   \n",
    "    create_dir(path_history)\n",
    "    \n",
    "    \n",
    "    # checkpoint necessary to save the parameters of the i-th network at the end of each epoch\n",
    "    my_checkpoint = [\n",
    "        ModelCheckpoint(filepath = path_parametres, monitor='val_loss', \n",
    "                        verbose=0, save_best_only=False, save_weights_only=False, \n",
    "                        mode='auto', save_freq='epoch')\n",
    "    ]\n",
    "\n",
    "    \n",
    "    # creation of networks with random initialization\n",
    "\n",
    "    model = Sequential([Dropout(0, input_dim=X_train.shape[1]),\n",
    "                    Dense(512, activation='relu'),\n",
    "                    Dropout(0),\n",
    "                    Dense(128,activation='relu'),\n",
    "                    Dropout(0),\n",
    "                    Dense(num_classes,activation='softmax')])\n",
    "\n",
    "    model.compile(optimizer='sgd',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    \n",
    "    # to save the network before starting training\n",
    "    model.save('E:/Fonti di Variazione/Rand. Init. + BS/epoca0-200/Checkpoint/model_NN'+str(i)+'_ep0.hdf5')\n",
    "    \n",
    "    \n",
    "    # network fit process with performance saving on the training set\n",
    "    pd.DataFrame(model.fit(X_train, y_train_dummy, validation_split=0.2, epochs=num_epochs, \n",
    "                batch_size=b_size, shuffle=True,callbacks=[my_checkpoint]).history).to_excel(path_history)\n",
    "    \n",
    "    \n",
    "    evaluated.append(model.evaluate(X_test, y_test_dummy))\n",
    "\n",
    "path_evaluation = 'E:/Fonti di Variazione/Rand. Init. + BS/epoca0-200/Evaluation/evaluated_models.xlsx'\n",
    "create_dir(path_evaluation)\n",
    "pd.DataFrame(evaluated).to_excel(path_evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustainable-johnston",
   "metadata": {},
   "source": [
    "## Rand. Init. + BS + Drop ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medieval-albuquerque",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluated = []            # list containing the accuracy and loss results of each network at the end of the training\n",
    "\n",
    "\n",
    "for i in range(num_NN):\n",
    "    # Checkpoint\n",
    "    cwd = os.getcwd()\n",
    "    \n",
    "    # path in which to save the parameters (weights and biases) of the i-th network\n",
    "    path_parametres = 'E:/Fonti di Variazione/Rand. Init. + BS + Drop/epoca0-200/Checkpoint/model_NN'+str(i)+'_ep{epoch:d}.hdf5'\n",
    "    \n",
    "    # path in which to save the loss and accuracy of each network at the end of each epoch\n",
    "    path_history = 'E:/Fonti di Variazione/Rand. Init. + BS + Drop/epoca0-200/loss_accuracy/history'+str(i)+'.xlsx'\n",
    "    \n",
    "    # creation of directories associated with paths\n",
    "    create_dir(path_parametres)   \n",
    "    create_dir(path_history)\n",
    "    \n",
    "    \n",
    "    # checkpoint necessary to save the parameters of the i-th network at the end of each epoch\n",
    "    my_checkpoint = [\n",
    "        ModelCheckpoint(filepath = path_parametres, monitor='val_loss', \n",
    "                        verbose=0, save_best_only=False, save_weights_only=False, \n",
    "                        mode='auto', save_freq='epoch')\n",
    "    ]\n",
    "\n",
    "\n",
    "    # creation of networks with random initialization\n",
    "\n",
    "    model = Sequential([Dropout(0.5, input_dim=X_train.shape[1]),\n",
    "                    Dense(512, activation='relu'),\n",
    "                    Dropout(0.5),\n",
    "                    Dense(128,activation='relu'),\n",
    "                    Dropout(0.5),\n",
    "                    Dense(num_classes,activation='softmax')])\n",
    "\n",
    "    model.compile(optimizer='sgd',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    \n",
    "    # to save the network before starting training\n",
    "    model.save('E:/Fonti di Variazione/Rand. Init. + BS + Drop/epoca0-200/Checkpoint/model_NN'+str(i)+'_ep0.hdf5')\n",
    "    \n",
    "    \n",
    "    # to modify the dropout rates of the loaded network starting from the fixed initialization\n",
    "    model.layers[0].rate = 0.5\n",
    "    model.layers[2].rate = 0.5\n",
    "    model.layers[4].rate = 0.5\n",
    "    \n",
    "    \n",
    "    # network fit process with performance saving on the training set\n",
    "    pd.DataFrame(model.fit(X_train, y_train_dummy, validation_split=0.2, epochs=num_epochs, \n",
    "                batch_size=b_size, shuffle=True,callbacks=[my_checkpoint]).history).to_excel(path_history)\n",
    "    \n",
    "    \n",
    "    evaluated.append(model.evaluate(X_test, y_test_dummy))\n",
    "\n",
    "path_evaluation = 'E:/Fonti di Variazione/Rand. Init. + BS + Drop/epoca0-200/Evaluation/evaluated_models.xlsx'\n",
    "create_dir(path_evaluation)\n",
    "pd.DataFrame(evaluated).to_excel(path_evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guided-submission",
   "metadata": {},
   "source": [
    "## Distorted + Rand. Init. + BS ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "missing-youth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the arguments\n",
    "rotation_range_val = 5\n",
    "width_shift_val = 0.25\n",
    "height_shift_val = 0.25\n",
    "shear_range_val = 45\n",
    "zoom_range_val=[1.0,1.5]\n",
    "# import relevant library\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "for i in range(num_NN):\n",
    "    # Checkpoint\n",
    "    cwd = os.getcwd()\n",
    "    \n",
    "    # path in which to save the parameters (weights and biases) of the i-th network\n",
    "    path_parametres = 'E:/Fonti di Variazione/Distorted + Rand. Init. + BS/epoca0-200/Checkpoint/model_NN'+str(i)+'_ep{epoch:d}.hdf5'\n",
    "    \n",
    "    # path in which to save the loss and accuracy of each network at the end of each epoch\n",
    "    path_history = 'E:/Fonti di Variazione/Distorted + Rand. Init. + BS/epoca0-200/loss_accuracy/history'+str(i)+'.xlsx'\n",
    "    \n",
    "    # creation of directories associated with paths\n",
    "    create_dir(path_parametres)   \n",
    "    create_dir(path_history)\n",
    "    \n",
    "    \n",
    "    # distorted dataset creation\n",
    "    datagen = ImageDataGenerator(rotation_range=rotation_range_val, width_shift_range=width_shift_val,\n",
    "                             height_shift_range=height_shift_val, zoom_range=zoom_range_val,\n",
    "                             shear_range=shear_range_val)\n",
    "\n",
    "    datagen.fit(X_train.reshape(X_train.shape[0], 28, 28, 1))\n",
    "    data_dist = datagen.flow(X_train.reshape(X_train.shape[0], 28, 28, 1),y_train.reshape(y_train.shape[0], 1),\n",
    "                      batch_size=X_train.shape[0],shuffle=False)\n",
    "    \n",
    "    X, y = data_dist.next()\n",
    "    X_dist = X.reshape(X.shape[0], 28, 28)\n",
    "    y_dist = y.reshape(y.shape[0])\n",
    "    \n",
    "    # data reshaping\n",
    "    X_dist = X_dist.reshape(X_dist.shape[0],28*28)\n",
    "\n",
    "    # data normalization\n",
    "    X_dist = X_dist/X_dist.max()\n",
    "    \n",
    "    # One-hot encoding of the label\n",
    "    y_dist_dummy = to_categorical(y_dist,num_classes)\n",
    "    y_val_dummy = to_categorical(y_val,num_classes)\n",
    "    y_test_dummy = to_categorical(y_test, num_classes)\n",
    "    \n",
    "    \n",
    "    # checkpoint necessary to save the parameters of the i-th network at the end of each epoch\n",
    "    my_checkpoint = [\n",
    "        ModelCheckpoint(filepath = path_parametres, monitor='val_loss', \n",
    "                        verbose=0, save_best_only=False, save_weights_only=False, \n",
    "                        mode='auto', save_freq='epoch')\n",
    "    ]\n",
    "    \n",
    "\n",
    "    # creation of networks with random initialization\n",
    "\n",
    "    model = Sequential([Dropout(0, input_dim=X_train.shape[1]),\n",
    "                    Dense(512, activation='relu'),\n",
    "                    Dropout(0),\n",
    "                    Dense(128,activation='relu'),\n",
    "                    Dropout(0),\n",
    "                    Dense(num_classes,activation='softmax')])\n",
    "\n",
    "    model.compile(optimizer='sgd',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    \n",
    "    # to save the network before starting training\n",
    "    model.save('E:/Fonti di Variazione/Distorted + Rand. Init. + BS/epoca0-200/Checkpoint/model_NN'+str(i)+'_ep0.hdf5')\n",
    "    \n",
    "    # network fit process with performance saving on the training set\n",
    "    pd.DataFrame(model.fit(X_dist, y_dist_dummy, validation_data=(X_val, y_val_dummy), epochs=num_epochs, \n",
    "                batch_size=b_size, shuffle=True,callbacks=[my_checkpoint]).history).to_excel(path_history)\n",
    "    \n",
    "    \n",
    "    evaluated.append(model.evaluate(X_test, y_test_dummy))\n",
    "\n",
    "path_evaluation = 'E:/Fonti di Variazione/Distorted + Rand. Init. + BS/epoca0-200/Evaluation/evaluated_models.xlsx'\n",
    "create_dir(path_evaluation)\n",
    "pd.DataFrame(evaluated).to_excel(path_evaluation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
